<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Bhuvvaan Punukolu </title> <meta name="author" content="Bhuvvaan Punukolu"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="Machine Learning, Data Science, Reinforcement Learning, Diffusion Models"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://bhuvvaan.github.io/publications/"> <script src="/assets/js/theme.js?f0ee386d3bce30bb640ff09232715426"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Bhuvvaan</span> Punukolu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/resume/">Resume </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/press/">Press </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description"></p> </header> <article> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">AAMAS</abbr> </div> <div id="agrawal2022shelfhelp" class="col-sm-8"> <div class="title">ShelfHelp: Empowering Humans to Perform Vision-Independent Manipulation Tasks with a Socially Assistive Robotic Cane</div> <div class="author"> Shivendra Agrawal, <a href="https://www.linkedin.com/in/suresh-n-nayak" rel="external nofollow noopener" target="_blank">Suresh Nayak</a>, <a href="https://ashutoshnaik.com/" rel="external nofollow noopener" target="_blank">Ashutosh Naik</a>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Bradley Hayes' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://shivendraagrawal.github.io/assets/pdf/agrawalShelfHelpaamas23.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://shivendraagrawal.github.io/projects/shelfhelp/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>The ability to shop independently, especially in grocery stores, is important for maintaining a high quality of life. This can be particularly challenging for people with visual impairments (PVI). Stores carry thousands of products, with approximately 30,000 new products introduced each year in the US market alone, presenting a challenge even for modern computer vision solutions. Through this work, we present a proof-of-concept socially assistive robotic system we call ShelfHelp, and propose novel technical solutions for enhancing instrumented canes traditionally meant for navigation tasks with additional capability within the domain of shopping. ShelfHelp includes a novel visual product locator algorithm designed for use in grocery stores and a novel planner that autonomously issues verbal manipulation guidance commands to guide the user during product retrieval. Through a human subjects study, we show the system’s success in locating and providing effective manipulation guidance to retrieve desired products with novice users. We compare two autonomous verbal guidance modes achieving comparable performance to a human assistance baseline and present encouraging findings that validate our system’s efficiency and effectiveness and through positive subjective metrics including competence, intelligence, and ease of use.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">agrawal2022shelfhelp</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ShelfHelp: Empowering Humans to Perform Vision-Independent Manipulation Tasks with a Socially Assistive Robotic Cane}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Agrawal, Shivendra and Nayak, Suresh and Naik, Ashutosh and Hayes, Bradley}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1514--1523}</span><span class="p">,</span>
  <span class="na">topic</span> <span class="p">=</span> <span class="s">{shelfhelp}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://shivendraagrawal.github.io/projects/shelfhelp/}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IROS</abbr> </div> <div id="agrawal2022shelf" class="col-sm-8"> <div class="title">ShelfHelp: Empowering Humans to Perform Vision-Independent Manipulation Tasks with a Socially Assistive Robotic Cane</div> <div class="author"> Shivendra Agrawal, and <a href="http://www.bradhayes.info/" rel="external nofollow noopener" target="_blank">Bradley Hayes</a> </div> <div class="periodical"> <em>In IROS 2022 SCIAR Workshop</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://shivendraagrawal.github.io/assets/pdf/shelfhelp_iros22_workshop.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://shivendraagrawal.github.io/projects/shelfhelp/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>The ability to shop independently, especially in grocery stores, is important for maintaining a high quality of life. This can be particularly challenging for people with visual impairments (PVI). Stores carry thousands of products, with approximately 30,000 new products introduced each year in the US market alone, presenting a challenge even for modern computer vision solutions. In this work we present our work-in-progress investigating technical solutions for enhancing instrumented canes traditionally meant for navigation tasks with capability within the domain of shopping. Our system includes a novel visual product search algorithm designed for use in the wild and a novel planner that autonomously issues verbal commands to guide the user in a reaching task to acquire them.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">agrawal2022shelf</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ShelfHelp: Empowering Humans to Perform Vision-Independent Manipulation Tasks with a Socially Assistive Robotic Cane}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Agrawal, Shivendra and Hayes, Bradley}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IROS 2022 SCIAR Workshop}</span><span class="p">,</span>
  <span class="na">topic</span> <span class="p">=</span> <span class="s">{shelfhelp}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://shivendraagrawal.github.io/assets/pdf/shelfhelp_iros22_workshop.pdf}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IROS</abbr> </div> <div id="agrawal2022novel" class="col-sm-8"> <div class="title">A Novel Perceptive Robotic Cane with Haptic Navigation for Enabling Vision-Independent Participation in the Social Dynamics of Seat Choice</div> <div class="author"> Shivendra Agrawal, <a href="https://www.ettawest.com/" rel="external nofollow noopener" target="_blank">Mary Etta West</a>, and <a href="http://www.bradhayes.info/" rel="external nofollow noopener" target="_blank">Bradley Hayes</a> </div> <div class="periodical"> <em>In Proceedings of the IEEERSJ International Conference on Intelligent Robots and Systems</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://www.cairo-lab.com/papers/iros22.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://shivendraagrawal.github.io/projects/social_guidance/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Goal-based navigation in public places is critical for independent mobility and for breaking barriers that exist for blind or visually impaired (BVI) people in a sight-centric society. Through this work we present a proof-of-concept system that autonomously leverages goal-based navigation assistance and perception to identify socially preferred seats and safely guide its user towards them in unknown indoor environments. The robotic system includes a camera, an IMU, vibrational motors, and a white cane, powered via a backpack-mounted laptop. The system combines techniques from computer vision, robotics, and motion planning with insights from psychology to perform 1) SLAM and object localization, 2) goal disambiguation and scoring, and 3) path planning and guidance. We introduce a novel 2-motor haptic feedback system on the cane’s grip for navigation assistance. Through a pilot user study, we show that the system is successful in classifying and providing haptic navigation guidance to socially preferred seats, while optimizing for users convenience, privacy, and intimacy in addition to increasing their confidence in independent navigation. The implications are encouraging as this technology, with careful design guided by the BVI community, can be adopted and further developed to be used with medical devices enabling the BVI population to better independently engage in socially dynamic situations like seat choice.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">agrawal2022novel</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Novel Perceptive Robotic Cane with Haptic Navigation for Enabling Vision-Independent Participation in the Social Dynamics of Seat Choice}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Agrawal, Shivendra and West, Mary Etta and Hayes, Bradley}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEERSJ International Conference on Intelligent Robots and Systems}</span><span class="p">,</span>
  <span class="na">topic</span> <span class="p">=</span> <span class="s">{social_guidance}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://www.cairo-lab.com/papers/iros22.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">HRI</abbr> </div> <div id="tabrez2019explanation" class="col-sm-8"> <div class="title">Explanation-based reward coaching to improve human performance via reinforcement learning</div> <div class="author"> <a href="https://www.linkedin.com/in/aaquib-tabrez" rel="external nofollow noopener" target="_blank">Aaquib Tabrez</a>, Shivendra Agrawal, and <a href="http://www.bradhayes.info/" rel="external nofollow noopener" target="_blank">Bradley Hayes</a> </div> <div class="periodical"> <em>In 2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Awarded</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://www.cairo-lab.com/papers/hri19.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://shivendraagrawal.github.io/projects/explainable_ai/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Best Paper Runner-up</p> </div> <div class="abstract hidden"> <p>For robots to effectively collaborate with humans, it is critical to establish a shared mental model amongst teammates. In the case of incongruous models, catastrophic failures may occur unless mitigating steps are taken. To identify and remedy these potential issues, we propose a novel mechanism for enabling an autonomous system to detect model disparity between itself and a human collaborator, infer the source of the disagreement within the model, evaluate potential consequences of this error, and finally, provide human-interpretable feedback to encourage model correction. This process effectively enables a robot to provide a human with a policy update based on perceived model disparity, reducing the likelihood of costly or dangerous failures during joint task execution. This paper makes two contributions at the intersection of explainable AI (xAI) and human-robot collaboration: 1) The Reward Augmentation and Repair through Explanation (RARE) framework for estimating task understanding and 2) A human subjects study illustrating the effectiveness of reward augmentation-based policy repair in a complex collaborative task.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tabrez2019explanation</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Explanation-based reward coaching to improve human performance via reinforcement learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tabrez, Aaquib and Agrawal, Shivendra and Hayes, Bradley}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{249--257}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">topic</span> <span class="p">=</span> <span class="s">{explainable_ai}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://www.cairo-lab.com/papers/hri19.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> <hr> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Bhuvvaan Punukolu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: February 05, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-resume",title:"Resume",description:"",section:"Navigation",handler:()=>{window.location.href="/resume/"}},{id:"nav-projects",title:"Projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-publications",title:"Publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-press",title:"Press",description:"",section:"Navigation",handler:()=>{window.location.href="/press/"}},{id:"nav-teaching",title:"Teaching",description:"",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"news-awarded-the-david-t-spalding-graduate-teaching-award",title:"Awarded the David T. Spalding Graduate Teaching Award.",description:"",section:"News"},{id:"news-recieved-an-award-for-our-poster-at-the-annual-research-expo-22-for-our-work",title:"Recieved an award for our poster at the Annual Research Expo 22 for...",description:"",section:"News"},{id:"news-demoed-our-work-at-the-amazon-re-mars-2022",title:"Demoed our work. at the Amazon re:MARS 2022.",description:"",section:"News"},{id:"news-presented-our-paper-on-faciliating-navigation-and-social-norm-adherence-for-the-bvi-at-iros-2022-kyoto",title:"Presented our paper on faciliating navigation and social norm adherence for the BVI...",description:"",section:"News"},{id:"news-presented-our-workshop-paper-on-our-new-system-called-shelfhelp-an-independent-grocery-shopping-assitant-for-the-bvi-at-iros-2022",title:"Presented our workshop paper on our new system called ShelfHelp: An independent grocery...",description:"",section:"News"},{id:"news-our-work-on-assiting-visually-impaired-people-to-perform-independent-grocery-shopping-got-accepted-for-aamas-2023-london",title:"Our work on assiting visually impaired people to perform independent grocery shopping got...",description:"",section:"News"},{id:"news-our-work-on-assisting-visually-impaired-people-to-perform-independent-grocery-shopping-was-a-winner-at-cu-boulder-s-annual-research-expo",title:"Our work on assisting visually impaired people to perform independent grocery shopping was...",description:"",section:"News"},{id:"news-awarded-the-aamas-student-scholarship-2023",title:"Awarded the AAMAS Student Scholarship 2023",description:"",section:"News"},{id:"news-represented-cu-robotics-research-at-leading-with-impact-event-in-denver-and-presented-our-work",title:"Represented CU Robotics research at Leading with Impact event in Denver and presented...",description:"",section:"News"},{id:"news-was-the-social-media-chair-for-the-human-robot-interaction-conference-2024-boulder-co",title:"Was the social media chair for the Human-Robot Interaction Conference 2024, Boulder, CO...",description:"",section:"News"},{id:"projects-an-arkit-app-to-help-people-with-learning-disabilities",title:"An ARKit app to help people with learning disabilities",description:"CSCI-5413 Augmented Reality Project using ARKit",section:"Projects",handler:()=>{window.location.href="/projects/textAR/"}},{id:"projects-robot-guide-dog-for-visually-impaired",title:"Robot guide dog for visually impaired",description:"Algorithmic Human-Robot Interaction class project Spring 2019",section:"Projects",handler:()=>{window.location.href="/projects/guide_dog/"}},{id:"projects-an-explainable-reinforcement-learning-approach-for-enabling-robots-to-coach-humans",title:"An Explainable Reinforcement Learning Approach for Enabling Robots to Coach Humans",description:"Best Technical Paper Runner-up @ Human-Robot Interaction conference 2019",section:"Projects",handler:()=>{window.location.href="/projects/explainable_ai/"}},{id:"projects-smart-cane-to-find-socially-preferred-seats",title:"Smart cane to find socially preferred seats",description:"2021-2022 - Published paper at IROS 2022",section:"Projects",handler:()=>{window.location.href="/projects/social_guidance/"}},{id:"projects-shelfhelp-an-assitive-robotic-system-to-support-grocery-shopping-for-bvi",title:"ShelfHelp - An assitive robotic system to support grocery shopping for BVI",description:"2023 - Published paper at AAMAS 23",section:"Projects",handler:()=>{window.location.href="/projects/shelfhelp/"}},{id:"projects-autonomous-anomaly-detection-and-explanation",title:"Autonomous Anomaly Detection and Explanation",description:"2024 - work done with NEC corporation",section:"Projects",handler:()=>{window.location.href="/projects/anomaly/"}},{id:"projects-shelfmcl-semantic-particle-filter-localization-with-low-cost-sensors",title:"ShelfMCL - Semantic particle filter localization with low-cost sensors",description:"2023-2024 - Paper in progress",section:"Projects",handler:()=>{window.location.href="/projects/shelfmcl/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%62%68%75%76%76%61%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/bhuvvaan","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/bhuvvaan-chandra","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/bhuvvaanchandra","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>