---
layout: about
title: About
permalink: /
subtitle: Data Scientist

profile:
  align: right
  image: profile_pic.jpg
  image_circular: false # crops the image to make it circular
  more_info: 

news: false # includes a list of news items
selected_papers: false # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

**About myself:** I am a 5th year CS Ph.D. student with Prof. Bradley Hayes at CU Boulder. I am interested in Robotics, Accessibility, and Human Robotics Interaction (HRI) and unifying them to create real-world Assistive Technology. My thesis involves developing robotic systems that can assist people with visual impairments in performing daily tasks more independently by providing long-term and fine-grain guidance.

**Other interests:** I enjoy playing tennis a lot (like a lot!!). On good sunny days, I also like to bike and run in the lovely city of Boulder. [(Bonus) Boulder Creek Path Fall view through my bike](https://youtube.com/playlist?list=PLEcdUQuIeys3P7XsRJMF3zriFkCa77X0j). I also like to explore local food and beverages.


**Minor-flex** - I have more than 51 millions views on my Google Map [contributions](https://www.google.com/maps/contrib/118433183916755884441/photos/) and have received a rather vibrant pair of socks from Google. And no, that was all I ever got from them.

**Research Abstract:** Recent advances in machine learning, computer vision, sensors, and computing have opened up opportunities to develop practical assistive technologies for real-world applications. However, these technologies have traditionally underutilized key algorithms and techniques from robotics. Concepts from robotics, such as persistent mapping, long-horizon planning, sequential decision-making, are essential for building effective assistive autonomy. My research explores some of these use cases and demonstrate how robotics principles can transform assistive technology into a tool for long-term guidance and independence.

**Research Summary:** 

* **Social Goal-Finding and Navigation:**  
  Our first work in this series uses insights from psychology to locate socially preferred seats (seats with high privacy and low intimacy). It aims to help blind and visually impaired (BVI) people to participate in the nuances of seat selection in public settings like sighted people. [Project page](https://shivendraagrawal.github.io/projects/social_guidance/)

* **Grasping Guidance with Markov Decision Process:**  
  Next work creates a system that can assist with an important sub-task in independent grocery shopping. Our system called *ShelfHelp* uses a novel computer vision algorithm to locate grocery items on a shelf and a Markov Decision Process-based fine-grain grasping guidance algorithm that issues verbal commands. [Project page](https://shivendraagrawal.github.io/projects/shelfhelp/) 

* **Semantic Monte Carlo Localization with minimal sensors:**  
  Estimating the location of an autonomous agent in cluttered, quasi-static environments such as grocery stores is challenging. We introduce a novel semantic localization algorithm that parses a semantically rich scene (think quasi-static, densely-packed aisles of shelves) and fuses the information with depth to estimate the 2D pose of the system with low-cost sensors. [Project page](https://shivendraagrawal.github.io/projects/shelfmcl/) 
