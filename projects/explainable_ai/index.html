<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> An Explainable Reinforcement Learning Approach for Enabling Robots to Coach Humans | Bhuvvaan Punukolu </title> <meta name="author" content="Bhuvvaan Punukolu"> <meta name="description" content="Best Technical Paper Runner-up @ Human-Robot Interaction conference 2019"> <meta name="keywords" content="Machine Learning, Data Science, Reinforcement Learning, Diffusion Models"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://bhuvvaan.github.io/projects/explainable_ai/"> <script src="/assets/js/theme.js?f0ee386d3bce30bb640ff09232715426"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Bhuvvaan</span> Punukolu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/resume/">Resume </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/press/">Press </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">An Explainable Reinforcement Learning Approach for Enabling Robots to Coach Humans</h1> <p class="post-description">Best Technical Paper Runner-up @ Human-Robot Interaction conference 2019</p> </header> <article> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Team members:
Aaquib Tabrez
Shivendra Agrawal
Bradley Hayes (Advisor)
</code></pre></div></div> <p><br><br> <strong>Paper</strong></p> <div class="publications"> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">HRI</abbr> </div> <div id="tabrez2019explanation" class="col-sm-8"> <div class="title">Explanation-based reward coaching to improve human performance via reinforcement learning</div> <div class="author"> <a href="https://www.linkedin.com/in/aaquib-tabrez" rel="external nofollow noopener" target="_blank">Aaquib Tabrez</a>, Shivendra Agrawal, and <a href="http://www.bradhayes.info/" rel="external nofollow noopener" target="_blank">Bradley Hayes</a> </div> <div class="periodical"> <em>In 2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Awarded</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://www.cairo-lab.com/papers/hri19.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://shivendraagrawal.github.io/projects/explainable_ai/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Best Paper Runner-up</p> </div> <div class="abstract hidden"> <p>For robots to effectively collaborate with humans, it is critical to establish a shared mental model amongst teammates. In the case of incongruous models, catastrophic failures may occur unless mitigating steps are taken. To identify and remedy these potential issues, we propose a novel mechanism for enabling an autonomous system to detect model disparity between itself and a human collaborator, infer the source of the disagreement within the model, evaluate potential consequences of this error, and finally, provide human-interpretable feedback to encourage model correction. This process effectively enables a robot to provide a human with a policy update based on perceived model disparity, reducing the likelihood of costly or dangerous failures during joint task execution. This paper makes two contributions at the intersection of explainable AI (xAI) and human-robot collaboration: 1) The Reward Augmentation and Repair through Explanation (RARE) framework for estimating task understanding and 2) A human subjects study illustrating the effectiveness of reward augmentation-based policy repair in a complex collaborative task.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tabrez2019explanation</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Explanation-based reward coaching to improve human performance via reinforcement learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tabrez, Aaquib and Agrawal, Shivendra and Hayes, Bradley}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{249--257}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">topic</span> <span class="p">=</span> <span class="s">{explainable_ai}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://www.cairo-lab.com/papers/hri19.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> <p>While much of the existing robotics literature focuses on making robots better at performing tasks, this work explores how we can use that proficiency and expertise to allow robots to provide personalized feedback with the goal of coaching humans to improve task performance. We investigate the challenges faced by robots while coaching humans (e.g., providing corrective feedback) during collaboration and how explainable AI can be leveraged to alleviate these challenges for fostering trust, teamwork, transparency.</p> <h2 id="robotic-coaching-in-hri"><strong>Robotic Coaching in HRI:</strong></h2> <p>In a collaborative task involving mixed human-robot teams, coaching helps to correct or improve the behavior of human collaborators by leveraging a robot teammate’s capabilities. Correcting or repairing the behavior of a teammate is even more important in critical applications where missteps could lead to financial losses, catastrophic failures, or risks of physical harm.</p> <p>In general, coaches engage in a range of communication skills including providing feedback, insights, and clarifications to help learners shift their perspectives about the task and thereby discover alternate and better ways to achieve their goals.</p> <p>Here, we focus on two key innovations from our work that contribute toward enabling proficient autonomous coaches:</p> <ol> <li>Reward Augmentation and Repair through Explanation (RARE): A novel coaching framework for understanding and correcting an agent’s decision-making process</li> <li>Human subjects study results showing that justification is an important, potentially necessary factor for convincing people to accept an agent’s advice.</li> </ol> <h2 id="reward-augmentation-and-repair-through-explanation-rare"><strong>Reward Augmentation and Repair through Explanation (RARE):</strong></h2> <p>Operating under the assumption that humans behave rationally to maximize reward when performing a task, our core insight is that <em>suboptimal behavior can be framed as an indicator of a malformed reward function</em> being used by an otherwise rational actor. Under this model, providing corrections to the actor’s reward function will allow them to “reconverge their policy” to perform more optimally.</p> <p>The process of Reward Augmentation and Repair through Explanation can be characterized through three interrelated challenges:</p> <ol> <li>Inferring the most likely reason for suboptimal behavior, by determining which parts of their reward function are malformed.</li> <li>Determining a policy trade-off between task execution and intervention, and</li> <li>Providing the necessary feedback and/or guidance to improve their performance.</li> </ol> <p>To make #1 tractable, we apply this work within a domain with sparse reward function (R(s) is only non-zero for a subset of states) and operate under the assumption that malformed reward functions occur only because the actor is missing information as opposed to using incorrect information (e.g., given actor reward function R and true reward function R<em>, R(s) = 0 for some values of s where R</em>(s) != 0. We do not address the more general problem of cases where R(s) = x and R*(s) = y for x != 0).</p> <p>Here, we explain the intuition behind RARE through an example. Consider a grid world below (Figure 1) with two terminal states having rewards of 10 and 100. Given a knowledgeable, rational individual we would expect to see them collect the bigger reward. However, if we see a person going after the sub-optimal reward (10), then the only way to preserve the rationality assumption is to assume that they must not know about the bigger reward — in other words, that their world model is incomplete/inaccurate. Our robotic coaching technique involves <strong><em>what</em></strong>, <strong><em>how</em></strong>, and <strong><em>when</em></strong> we can tell the human collaborator about the inaccuracies in their task comprehension (reward function) such that they will have the option to improve their behavior (policy).</p> <p align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/RARE/RARE-explain.gif" sizes="95vw"></source> <img src="/assets/img/blog/RARE/RARE-explain.gif" class="img-fluid rounded" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <br><em><b>Figure 1: RARE framework estimating the missing reward factor (100) based on human behavior (red line)</b></em> </p> <p>Let’s look at the process of estimating the human collaborator’s mental model in the same example. In the first step, the robot observes the human at state S1 (Figure 2). Note that each state is composed of the world state and additional latent boolean variables that indicate the robot’s knowledge of the human’s awareness of each of the reward factors (r1, r2) in the world. At this stage, the robot doesn’t know if the human knows about either of the rewards. But at timestep 2, the robot can gather more information after observing the human’s next action. At timestep 2 the human is at state S2, and the robot still doesn’t know enough to take any action. But if the human goes to state S3 at timestep 3, the robot can infer that the human isn’t aware of the greater reward at the upper right corner. This answers the “<strong><em>what</em></strong>’’ (missing reward r2), but not the “<strong><em>when</em></strong>” or the “<strong><em>how</em></strong>”.</p> <p>To address the “<strong><em>when</em></strong>”, we model the problem as a <a style="color:blue" href="https://people.csail.mit.edu/lpk/papers/aij98-pomdp.pdf" rel="external nofollow noopener" target="_blank">POMDP</a>. In this case, we have a robot who is coaching while collaborating (i.e., the robot is able to participate in tasks as well as coach through explanation). Latent variables in the POMDP state space consist of Boolean variables indicating whether or not the human’s reward function contains each non-zero entry in R. While the robot’s understanding of the human’s mental model of reward is latent because we don’t actually know them beforehand and can’t observe them directly, we can gather clues to reduce state uncertainty by watching the human’s policy in action. By solving the POMDP, the robotic coach can successfully resolve when to take task-productive actions from the domain or communicative, reward repairing actions.</p> <p align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/RARE/state-transition-diagram.gif" sizes="95vw"></source> <img src="/assets/img/blog/RARE/state-transition-diagram.gif" class="img-fluid rounded" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <br><em><b>Figure 2: Procedural visualization of the estimation of human collaborator's mental model by the robot using RARE.</b></em> </p> <p>Finally, we are left with the challenge of resolving “<strong><em>how</em></strong>” the robot should convey the missing reward information to the human. While there are a number of choices, we investigate two classes of feedback: unjustified advice and advice with justification. An example of unjustified advice is to have the robot indicate the sub-optimality of the human’s action to encourage exploration (e.g., “If you do that you won’t get the best reward” or “That’s a bad move”). Advice with justification would entail having the robot provide a description of the reward’s location to supplement the advice (e.g., “If you do that won’t get the best reward. There’s a better reward in the top right corner.” or “If you do that you won’t get the best reward. There’s a large negative reward in region X.”).</p> <p align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/RARE/policy-correction.gif" sizes="95vw"></source> <img src="/assets/img/blog/RARE/policy-correction.gif" class="img-fluid rounded" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <br><em><b>Figure 3 Caption: RARE enables robotic coaches to repair sub-optimal policy by informing the human about the missing reward from their mental model.</b></em> </p> <p><i>Note: We use previous work from <a style="color:blue" href="http://www.bradhayes.info/papers/hri17.pdf" rel="external nofollow noopener" target="_blank">Hayes and Shah</a> to describe state regions for natural language policy updates. </i></p> <figure class="half" style="display:flex"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/RARE/fig_option1.jpg" sizes="95vw"></source> <img src="/assets/img/blog/RARE/fig_option1.jpg" class="img-fluid rounded" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/RARE/fig_option2.jpg" sizes="95vw"></source> <img src="/assets/img/blog/RARE/fig_option2.jpg" class="img-fluid rounded" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </figure> <p align="center"> <em><b>Figure 4: Options for providing reward coaching to improve the sub-optimal policy: 1) advice without justification and 2) advice with justification</b></em> </p> <h4 id="we-performed-a-user-study-to-test-the-strength-of-rare-and-to-figure-out-the-best-option-to-convey-the-missing-reward">We performed a user study to test the strength of RARE and to figure out the best option to convey the missing reward.</h4> <h2 id="experimental-evaluation-and-results"><strong>Experimental evaluation and results:</strong></h2> <figure class="half" style="display:flex"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/RARE/fig_user_study1.jpg" sizes="95vw"></source> <img src="/assets/img/blog/RARE/fig_user_study1.jpg" class="img-fluid rounded" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/RARE/fig_user_study2.jpg" sizes="95vw"></source> <img src="/assets/img/blog/RARE/fig_user_study2.jpg" class="img-fluid rounded" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </figure> <figure class="half" style="display:flex"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/RARE/fig_user_study3.jpg" sizes="95vw"></source> <img src="/assets/img/blog/RARE/fig_user_study3.jpg" class="img-fluid rounded" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/RARE/fig_user_study4.jpg" sizes="95vw"></source> <img src="/assets/img/blog/RARE/fig_user_study4.jpg" class="img-fluid rounded" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </figure> <p align="center"> <em><b> Figure 5 (Clockwise from top left) </b> <ol> <li>Our experimental setup where the robot and the human collaborator are simultaneously and collaboratively solving a modified Sudoku game.</li> <li>We conducted a between-subject study with 3 test conditions: control, justification, and no interruption.</li> <li>Our modified Sudoku game uses a 6x6 grid with 6 different colors available (instead of 9 numbers). The goal is to fill this grid collaboratively such that no rules are violated. Each player fills cells on the board from right to left, nearest to the farthest, while filling their share of the rows. There are no turns and a player is supposed to play their turn whenever they are ready. </li> <li>We can clearly see that due to these constraints, past moves have long term consequences for the success of the game, making this an incredibly difficult reward function to keep in working memory. Consequently, a player also needs to update their policy/moves based on the collaborator's moves making, the game even more cognitively demanding.</li> </ol> </em> </p> <p>To determine the effectiveness of RARE and the usefulness of justification, we came up with a difficult game for people to play with a robot. We adapted the game of Sudoku and made it more challenging by putting it in a collaborative setting (see the above Figure 5 for details).</p> <p>The subjective evaluation of our study shows that robots that provide justification for advice are perceived as more useful, helpful, and intelligent coaches compared to the control condition in which no explanation for failure was given. Another surprising result which emerged during the study was that people barely followed the robot’s advice (only 20% listened, while the other 80% of participants ignored advice and failed to solve the puzzle) when the robot did not provide any justification for its recommendation. In contrast, nearly everyone followed the robot’s advice (80% successfully completed the game) when justification was provided. Furthermore, we saw from open questionnaire responses that the participants had a more positive user experience in the justification condition.</p> <p align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/RARE/quotes.gif" sizes="95vw"></source> <img src="/assets/img/blog/RARE/quotes.gif" class="img-fluid rounded" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <br><em><b>Figure 6: Some of the participant feedback from the user study showing the contrast between the user experience in both condition</b></em> </p> <h2 id="conclusion"><strong>Conclusion:</strong></h2> <p>To summarize, we developed a novel framework enabling an expert robot to coach a novice collaborator within a reinforcement learning context. We evaluated our framework in a challenging collaborative cognitive game with a human and a robot. We found that using justification during coaching makes robots more useful, helpful, and intelligent coaches.</p> <hr> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Bhuvvaan Punukolu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: February 05, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-resume",title:"Resume",description:"",section:"Navigation",handler:()=>{window.location.href="/resume/"}},{id:"nav-projects",title:"Projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-publications",title:"Publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-press",title:"Press",description:"",section:"Navigation",handler:()=>{window.location.href="/press/"}},{id:"nav-teaching",title:"Teaching",description:"",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"news-awarded-the-david-t-spalding-graduate-teaching-award",title:"Awarded the David T. Spalding Graduate Teaching Award.",description:"",section:"News"},{id:"news-recieved-an-award-for-our-poster-at-the-annual-research-expo-22-for-our-work",title:"Recieved an award for our poster at the Annual Research Expo 22 for...",description:"",section:"News"},{id:"news-demoed-our-work-at-the-amazon-re-mars-2022",title:"Demoed our work. at the Amazon re:MARS 2022.",description:"",section:"News"},{id:"news-presented-our-paper-on-faciliating-navigation-and-social-norm-adherence-for-the-bvi-at-iros-2022-kyoto",title:"Presented our paper on faciliating navigation and social norm adherence for the BVI...",description:"",section:"News"},{id:"news-presented-our-workshop-paper-on-our-new-system-called-shelfhelp-an-independent-grocery-shopping-assitant-for-the-bvi-at-iros-2022",title:"Presented our workshop paper on our new system called ShelfHelp: An independent grocery...",description:"",section:"News"},{id:"news-our-work-on-assiting-visually-impaired-people-to-perform-independent-grocery-shopping-got-accepted-for-aamas-2023-london",title:"Our work on assiting visually impaired people to perform independent grocery shopping got...",description:"",section:"News"},{id:"news-our-work-on-assisting-visually-impaired-people-to-perform-independent-grocery-shopping-was-a-winner-at-cu-boulder-s-annual-research-expo",title:"Our work on assisting visually impaired people to perform independent grocery shopping was...",description:"",section:"News"},{id:"news-awarded-the-aamas-student-scholarship-2023",title:"Awarded the AAMAS Student Scholarship 2023",description:"",section:"News"},{id:"news-represented-cu-robotics-research-at-leading-with-impact-event-in-denver-and-presented-our-work",title:"Represented CU Robotics research at Leading with Impact event in Denver and presented...",description:"",section:"News"},{id:"news-was-the-social-media-chair-for-the-human-robot-interaction-conference-2024-boulder-co",title:"Was the social media chair for the Human-Robot Interaction Conference 2024, Boulder, CO...",description:"",section:"News"},{id:"projects-an-arkit-app-to-help-people-with-learning-disabilities",title:"An ARKit app to help people with learning disabilities",description:"CSCI-5413 Augmented Reality Project using ARKit",section:"Projects",handler:()=>{window.location.href="/projects/textAR/"}},{id:"projects-robot-guide-dog-for-visually-impaired",title:"Robot guide dog for visually impaired",description:"Algorithmic Human-Robot Interaction class project Spring 2019",section:"Projects",handler:()=>{window.location.href="/projects/guide_dog/"}},{id:"projects-an-explainable-reinforcement-learning-approach-for-enabling-robots-to-coach-humans",title:"An Explainable Reinforcement Learning Approach for Enabling Robots to Coach Humans",description:"Best Technical Paper Runner-up @ Human-Robot Interaction conference 2019",section:"Projects",handler:()=>{window.location.href="/projects/explainable_ai/"}},{id:"projects-smart-cane-to-find-socially-preferred-seats",title:"Smart cane to find socially preferred seats",description:"2021-2022 - Published paper at IROS 2022",section:"Projects",handler:()=>{window.location.href="/projects/social_guidance/"}},{id:"projects-shelfhelp-an-assitive-robotic-system-to-support-grocery-shopping-for-bvi",title:"ShelfHelp - An assitive robotic system to support grocery shopping for BVI",description:"2023 - Published paper at AAMAS 23",section:"Projects",handler:()=>{window.location.href="/projects/shelfhelp/"}},{id:"projects-autonomous-anomaly-detection-and-explanation",title:"Autonomous Anomaly Detection and Explanation",description:"2024 - work done with NEC corporation",section:"Projects",handler:()=>{window.location.href="/projects/anomaly/"}},{id:"projects-shelfmcl-semantic-particle-filter-localization-with-low-cost-sensors",title:"ShelfMCL - Semantic particle filter localization with low-cost sensors",description:"2023-2024 - Paper in progress",section:"Projects",handler:()=>{window.location.href="/projects/shelfmcl/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%62%68%75%76%76%61%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/bhuvvaan","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/bhuvvaan-chandra","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/bhuvvaanchandra","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>